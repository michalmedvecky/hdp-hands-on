<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>{{ hdfs_replication }}</value>
    </property>
{% if 'hdfs-namenode' in group_names %}
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file://{{ hdfs_namenode_name_dir }}</value>
    </property>
    <property>
        <name>dfs.hosts.exclude</name>
        <value>{{hadoop_conf_dir }}/hdfs.exclude</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-bind-host</name>
        <value>0.0.0.0</value>
    </property>
{% endif %}
   <property>
     <name>dfs.ha.automatic-failover.enabled</name>
     <value>true</value>
   </property>
    <property>
        <name>dfs.nameservices</name>
        <value>{{ hadoop_cluster }}</value>
    </property>
    <property>
        <name>dfs.ha.namenodes.{{ hadoop_cluster }}</name>
        <value>{%- set comma = joiner(",") -%}
{%- for host in groups[hadoop_cluster] if host in groups['hdfs-namenode'] -%}
{{- comma() }}{{ hostvars[host]['hadoop_hostname']|default(hostvars[host]['inventory_hostname']) }}
{%- endfor -%}</value>
    </property>
{% for host in groups[hadoop_cluster] if host in groups['hdfs-namenode']%}
    <property>
      <name>dfs.namenode.rpc-address.{{ hadoop_cluster }}.{{ hostvars[host]['hadoop_hostname']|default(hostvars[host]['inventory_hostname']) }}</name>
      <value>{{ hostvars[host]['hadoop_hostname']|default(hostvars[host]['inventory_hostname']) }}:8020</value>
    </property>
{% endfor %}
    <property>
      <name>dfs.namenode.shared.edits.dir</name>
      <value>qjournal://{%- set comma = joiner(";") -%}
{%- for host in groups[hadoop_cluster] if host in groups['hdfs-journalnode'] -%}
{{- comma() }}{{ hostvars[host]['hadoop_hostname']|default(hostvars[host]['inventory_hostname']) }}:8485
{%- endfor -%}/{{ hadoop_cluster }}</value>
    </property>
    <property>
      <name>dfs.client.failover.proxy.provider.{{ hadoop_cluster }}</name>
      <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
{% if 'hdfs-namenode' in group_names %}
    <property>
      <name>dfs.ha.fencing.methods</name>
      <value>sshfence
             shell(service hdfs-namenode stop)</value>
    </property>
    <property>
      <name>dfs.ha.fencing.ssh.private-key-files</name>
      <value>/home/hdfs/.ssh/id_rsa</value>
    </property>
    <property>
        <name>dfs.namenode.checkpoint.dir</name>
        <value>file://{{ hdfs_namenode_checkpoint_dir }}</value>
    </property>
{% endif %}

{% if 'hdfs-journalnode' in group_names %}
    <property>
      <name>dfs.journalnode.edits.dir</name>
      <value>{{ hdfs_journalnode_data_dir }}</value>
    </property>
{% endif %}
{% if 'hdfs-datanode' in group_names %}
    <property>
        <name>dfs.datanode.data.dir</name>
{% if hdfs_datanode_data_dirs is defined %}
{%- set comma = joiner(",") -%}
	<value>{%- for dir in hdfs_datanode_data_dirs -%}
{{- comma() }}file://{{ dir }}
{%- endfor -%}</value>
{% else %}
<value>file://{{ hdfs_datanode_data_dir }}</value>
{% endif %}
    </property>
{% endif %}
    <property>
        <name>dfs.permissions.superusergroup</name>
        <value>hadoop</value>
    </property>
{% if 'hdfs-datanode' in group_names %}
    <property>
        <name>dfs.support.append</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.datanode.max.xcievers</name>
        <value>4096</value>
    </property>
    <property>
        <name>dfs.datanode.max.transfer.threads</name>
        <value>6144</value>
    </property>
    <property>
        <name>dfs.domain.socket.path</name>
        <value>/var/run/hdfs/dn_socket</value>
    </property>
    <property>
        <name>dfs.client.read.shortcircuit</name>
        <value>true</value>
    </property>
{% endif %}
</configuration>
